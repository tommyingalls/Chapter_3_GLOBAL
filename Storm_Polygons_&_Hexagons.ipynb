{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb1c990-e5a2-45b8-ad59-dc684cbc454f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0327b8-f82a-4a8f-8a81-bfb2f49c630f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point, Polygon, box, LineString, MultiPolygon\n",
    "from shapely.ops import transform, unary_union\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "from pyproj import CRS\n",
    "import math\n",
    "import h3\n",
    "\n",
    "import pyproj\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b87ccf-0b0d-4dd3-a602-22c4c0c33469",
   "metadata": {},
   "source": [
    "### Storm polygon generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4e213-caab-46aa-be13-cc9e158da8d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_hurricane_polygon_shapefiles(csv_path, output_dir):\n",
    "    \"\"\"\n",
    "    Creates polygon shapefiles for hurricane tracks using radius data.\n",
    "    Processes ALL storms without filtering by region or removing any rows.\n",
    "    \"\"\"\n",
    "    # Read the CSV file without dropping any rows\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Read CSV with {len(df)} rows, {len(df['SID'].unique())} unique storms\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique storm IDs\n",
    "    unique_sids = df['SID'].unique()\n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for sid in unique_sids:\n",
    "        # Filter data for this specific storm\n",
    "        sid_df = df[df['SID'] == sid].copy().reset_index(drop=True)\n",
    "        print(f\"Processing SID: {sid} - Points: {len(sid_df)}\")\n",
    "        \n",
    "        # Process all storms with at least one point\n",
    "        if len(sid_df) >= 1:\n",
    "            # Store individual point polygons\n",
    "            point_polygons = []\n",
    "            \n",
    "            for i in range(len(sid_df)):\n",
    "                try:\n",
    "                    row = sid_df.iloc[i]\n",
    "                    \n",
    "                    # Get center point coordinates\n",
    "                    center_lon = float(row['LON'])\n",
    "                    center_lat = float(row['LAT'])\n",
    "                    \n",
    "                    # Default radius (in nautical miles) if we can't get valid values\n",
    "                    ne_radius = se_radius = sw_radius = nw_radius = 10.0\n",
    "                    \n",
    "                    # Try to get radius values safely - if any fail, keep the default\n",
    "                    try:\n",
    "                        if 'USA_R50_NE' in row and pd.notna(row['USA_R50_NE']):\n",
    "                            val = str(row['USA_R50_NE']).strip()\n",
    "                            if val != '':\n",
    "                                ne_radius = float(val)\n",
    "                    except: pass\n",
    "                    \n",
    "                    try:\n",
    "                        if 'USA_R50_SE' in row and pd.notna(row['USA_R50_SE']):\n",
    "                            val = str(row['USA_R50_SE']).strip()\n",
    "                            if val != '':\n",
    "                                se_radius = float(val)\n",
    "                    except: pass\n",
    "                    \n",
    "                    try:\n",
    "                        if 'USA_R50_SW' in row and pd.notna(row['USA_R50_SW']):\n",
    "                            val = str(row['USA_R50_SW']).strip()\n",
    "                            if val != '':\n",
    "                                sw_radius = float(val)\n",
    "                    except: pass\n",
    "                    \n",
    "                    try:\n",
    "                        if 'USA_R50_NW' in row and pd.notna(row['USA_R50_NW']):\n",
    "                            val = str(row['USA_R50_NW']).strip()\n",
    "                            if val != '':\n",
    "                                nw_radius = float(val)\n",
    "                    except: pass\n",
    "                    \n",
    "                    # Convert radius from nautical miles to degrees\n",
    "                    lat_scale = 1/60.0  # 1 degree = 60 nautical miles\n",
    "                    lat_radians = math.radians(center_lat)\n",
    "                    \n",
    "                    # Avoid division by zero for poles\n",
    "                    cos_lat = math.cos(lat_radians)\n",
    "                    if abs(cos_lat) < 0.001:\n",
    "                        cos_lat = 0.001 if cos_lat >= 0 else -0.001\n",
    "                    \n",
    "                    lon_scale = 1/(60.0 * cos_lat)\n",
    "                    \n",
    "                    # Create polygon points\n",
    "                    polygon_points = []\n",
    "                    num_segments = 36  # For a smooth circle-like shape\n",
    "                    \n",
    "                    for j in range(num_segments):\n",
    "                        angle = j * (2 * math.pi / num_segments)\n",
    "                        \n",
    "                        # Determine radius based on the angle\n",
    "                        if 0 <= angle < math.pi/2:  # NE\n",
    "                            radius = ne_radius\n",
    "                        elif math.pi/2 <= angle < math.pi:  # SE\n",
    "                            radius = se_radius\n",
    "                        elif math.pi <= angle < 3*math.pi/2:  # SW\n",
    "                            radius = sw_radius\n",
    "                        else:  # NW\n",
    "                            radius = nw_radius\n",
    "                        \n",
    "                        # Calculate point coordinates\n",
    "                        point_lon = center_lon + radius * lon_scale * math.cos(angle)\n",
    "                        point_lat = center_lat + radius * lat_scale * math.sin(angle)\n",
    "                        polygon_points.append((point_lon, point_lat))\n",
    "                    \n",
    "                    # Close the polygon\n",
    "                    polygon_points.append(polygon_points[0])\n",
    "                    \n",
    "                    # Create polygon for this point\n",
    "                    point_polygon = Polygon(polygon_points)\n",
    "                    if point_polygon.is_valid:\n",
    "                        point_polygons.append(point_polygon)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing point {i} for SID {sid}: {str(e)}\")\n",
    "            \n",
    "            # Create the final polygon if we have any valid points\n",
    "            if point_polygons:\n",
    "                try:\n",
    "                    # Choose polygon creation method\n",
    "                    if len(point_polygons) == 1:\n",
    "                        # Just use the single polygon\n",
    "                        storm_polygon = point_polygons[0]\n",
    "                        method_used = \"Single Point\"\n",
    "                    else:\n",
    "                        # Try to create a union of all polygons\n",
    "                        try:\n",
    "                            storm_polygon = unary_union(point_polygons)\n",
    "                            method_used = \"Point Union\"\n",
    "                            \n",
    "                            # If we got a MultiPolygon, take the largest part\n",
    "                            if isinstance(storm_polygon, MultiPolygon):\n",
    "                                largest_polygon = max(storm_polygon.geoms, key=lambda p: p.area)\n",
    "                                storm_polygon = largest_polygon\n",
    "                                method_used = \"Largest Polygon\"\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            # Fallback to convex hull if union fails\n",
    "                            print(f\"  Union failed for SID {sid}: {str(e)}\")\n",
    "                            all_points = []\n",
    "                            for poly in point_polygons:\n",
    "                                all_points.extend(list(poly.exterior.coords))\n",
    "                            storm_polygon = Polygon(all_points).convex_hull\n",
    "                            method_used = \"Convex Hull\"\n",
    "                    \n",
    "                    # Create metadata from first point\n",
    "                    metadata = {}\n",
    "                    for col in sid_df.columns:\n",
    "                        if col not in ['LON', 'LAT', 'geometry']:\n",
    "                            val = sid_df.iloc[0].get(col, '')\n",
    "                            if pd.notna(val):\n",
    "                                metadata[col] = val\n",
    "                    \n",
    "                    # Add some useful metadata\n",
    "                    metadata['SID'] = sid\n",
    "                    metadata['method'] = method_used\n",
    "                    \n",
    "                    # Add some calculated metrics if available\n",
    "                    try:\n",
    "                        if 'USA_WIND' in sid_df.columns:\n",
    "                            winds = [float(w) for w in sid_df['USA_WIND'] if pd.notna(w)]\n",
    "                            if winds:\n",
    "                                metadata['max_wind'] = max(winds)\n",
    "                    except: pass\n",
    "                    \n",
    "                    try:\n",
    "                        if 'USA_PRES' in sid_df.columns:\n",
    "                            pres = [float(p) for p in sid_df['USA_PRES'] if pd.notna(p)]\n",
    "                            if pres:\n",
    "                                metadata['min_pres'] = min(pres)\n",
    "                    except: pass\n",
    "                    \n",
    "                    try:\n",
    "                        if 'USA_SSHS' in sid_df.columns:\n",
    "                            sshs = [float(s) for s in sid_df['USA_SSHS'] if pd.notna(s) and float(s) >= 0]\n",
    "                            if sshs:\n",
    "                                metadata['max_sshs'] = max(sshs)\n",
    "                    except: pass\n",
    "                    \n",
    "                    # Clean up metadata keys for shapefile compatibility\n",
    "                    clean_metadata = {}\n",
    "                    for key, value in metadata.items():\n",
    "                        # Limit to 10 characters, alphanumeric + underscore\n",
    "                        new_key = ''.join(c for c in key if c.isalnum() or c == '_')[:10]\n",
    "                        clean_metadata[new_key] = value\n",
    "                    \n",
    "                    # Create GeoDataFrame\n",
    "                    polygon_gdf = gpd.GeoDataFrame(\n",
    "                        [clean_metadata],\n",
    "                        geometry=[storm_polygon],\n",
    "                        crs=\"EPSG:4326\"\n",
    "                    )\n",
    "                    \n",
    "                    # Write to shapefile\n",
    "                    output_file = os.path.join(output_dir, f'{sid}_hurricane_polygon.shp')\n",
    "                    polygon_gdf.to_file(output_file)\n",
    "                    print(f\"  Connected polygon shapefile created for SID {sid}: {output_file}\")\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to create shapefile for SID {sid}: {str(e)}\")\n",
    "                    failed_count += 1\n",
    "            else:\n",
    "                print(f\"  No valid polygons created for SID {sid}\")\n",
    "                failed_count += 1\n",
    "        else:\n",
    "            print(f\"  No points found for SID {sid}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"\\nProcessed {processed_count} storms successfully, {failed_count} failed\")\n",
    "    print(\"All connected polygon shapefiles have been generated.\")\n",
    "\n",
    "# Example usage\n",
    "csv_path = 'global_storm_data.csv'\n",
    "output_dir = 'hurricane_connected_polygon_shapefiles_fixed'\n",
    "create_hurricane_polygon_shapefiles(csv_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64567849-8510-4cd1-9615-74cbea58d0a4",
   "metadata": {},
   "source": [
    "### Sort hurricane tracks to each basin/subbasin they pass through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585a007-49d2-4eeb-925d-4c4097ada0ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories containing the shapefiles\n",
    "hurricane_dir = 'hurricane_connected_polygon_shapefiles_fixed' ## 'hurricane_line_shapefiles2' ## hurricane_line_shapefiles3\n",
    "polygon_dir = 'polygon_shapefiles' ## 'polygon_shapefiles'\n",
    "\n",
    "# List of polygon identifiers\n",
    "polygon_names = ['MOZ_MAD', 'BAY_BEN', 'EST_AUS', 'WST_AUS', \n",
    "                 'CAR_SEA', 'GUF_MEX', 'STN_ISL', 'PHI_PIN', 'EQU_CON']\n",
    "\n",
    "# Create directories for each polygon\n",
    "output_dirs = {name: os.path.join('categorized_polygon_shapefiles2', name) for name in polygon_names}  ## 'categorized_line_shapefiles2'\n",
    "for directory in output_dirs.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Load all polygon shapefiles into a single GeoDataFrame with an identifier column\n",
    "polygon_files = [f for f in os.listdir(polygon_dir) if f.endswith('.shp')]\n",
    "polygons_list = []\n",
    "\n",
    "for file in polygon_files:\n",
    "    name = os.path.splitext(file)[0]\n",
    "    if name in polygon_names:\n",
    "        gdf = gpd.read_file(os.path.join(polygon_dir, file))\n",
    "        gdf['polygon_name'] = name  # Add an identifier column\n",
    "        polygons_list.append(gdf)\n",
    "\n",
    "polygons_gdf = gpd.GeoDataFrame(pd.concat(polygons_list, ignore_index=True))\n",
    "\n",
    "# Ensure CRS is consistent\n",
    "polygons_gdf = polygons_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Process each hurricane shapefile\n",
    "hurricane_files = [f for f in os.listdir(hurricane_dir) if f.endswith('.shp')]\n",
    "\n",
    "for hurricane_file in hurricane_files:\n",
    "    # Load the hurricane shapefile\n",
    "    hurricane_gdf = gpd.read_file(os.path.join(hurricane_dir, hurricane_file))\n",
    "    \n",
    "    # Ensure CRS is consistent\n",
    "    hurricane_gdf = hurricane_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Check intersections with polygons\n",
    "    intersections = polygons_gdf[polygons_gdf.geometry.intersects(hurricane_gdf.geometry.union_all())]\n",
    "    \n",
    "    if not intersections.empty:\n",
    "        # Save to the folder corresponding to the first intersecting polygon\n",
    "        for _, row in intersections.iterrows():\n",
    "            polygon_name = row['polygon_name']\n",
    "            output_path = os.path.join(output_dirs[polygon_name], hurricane_file)\n",
    "            hurricane_gdf.to_file(output_path)\n",
    "            print(f\"{hurricane_file} saved to {polygon_name} folder.\")\n",
    "    else:\n",
    "        print(f\"{hurricane_file} does not intersect with any polygon.\")\n",
    "\n",
    "print(\"Categorization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c46e3f-2f4e-4bac-874c-f034720dc037",
   "metadata": {},
   "source": [
    "### Combine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec29348-8325-44ad-b47a-5d43337f2849",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_shapefiles_in_categories(categorized_dir):\n",
    "    # Get all subdirectories in the categorized directory\n",
    "    categories = [d for d in os.listdir(categorized_dir) \n",
    "                  if os.path.isdir(os.path.join(categorized_dir, d))]\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(categorized_dir, category)\n",
    "        \n",
    "        # Find all shapefiles in the category\n",
    "        shapefiles = [f for f in os.listdir(category_path) if f.endswith('.shp')]\n",
    "        \n",
    "        # Collect GeoDataFrames\n",
    "        gdfs = []\n",
    "        for shapefile in shapefiles:\n",
    "            gdf = gpd.read_file(os.path.join(category_path, shapefile))\n",
    "            gdfs.append(gdf)\n",
    "        \n",
    "        # Combine shapefiles\n",
    "        if gdfs:\n",
    "            combined_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "            combined_gdf.crs = \"EPSG:4326\"\n",
    "            \n",
    "            # Save combined shapefile\n",
    "            output_path = os.path.join(categorized_dir, f'{category}_combined_poly.shp')\n",
    "            combined_gdf.to_file(output_path)\n",
    "            print(f\"Combined shapefile created for {category}: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "categorized_dir = 'categorized_polygon_shapefiles2'\n",
    "combine_shapefiles_in_categories(categorized_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5a5f5-f9aa-40c8-bc7c-1315cab3fb71",
   "metadata": {},
   "source": [
    "# Hexgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd06e2-d78a-4d0b-b335-dad5029ff6b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_hexagon(center_x, center_y, size):\n",
    "    \"\"\"Create a hexagon of given size centered at (center_x, center_y)\"\"\"\n",
    "    # Start from top point and work clockwise\n",
    "    angles_deg = np.array([30, 90, 150, 210, 270, 330])\n",
    "    angles_rad = np.radians(angles_deg)\n",
    "    x = center_x + size * np.cos(angles_rad)\n",
    "    y = center_y + size * np.sin(angles_rad)\n",
    "    return Polygon(zip(x, y))\n",
    "\n",
    "def generate_hex_grid(bounds, target_area_km2):\n",
    "    \"\"\"Generate a continuous hexagonal grid within given bounds\"\"\"\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    \n",
    "    # Calculate center point for projections\n",
    "    center_lat = (miny + maxy) / 2\n",
    "    center_lon = (minx + maxx) / 2\n",
    "    \n",
    "    # Create an equal area projection centered on the region\n",
    "    proj_string = f\"+proj=aea +lat_1={center_lat-10} +lat_2={center_lat+10} +lat_0={center_lat} +lon_0={center_lon} +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n",
    "    \n",
    "    # Create transformation functions\n",
    "    project = pyproj.Transformer.from_crs('EPSG:4326', proj_string, always_xy=True).transform\n",
    "    unproject = pyproj.Transformer.from_crs(proj_string, 'EPSG:4326', always_xy=True).transform\n",
    "    \n",
    "    # Transform bounds to projected space\n",
    "    bbox = transform(project, box(minx, miny, maxx, maxy))\n",
    "    proj_bounds = bbox.bounds\n",
    "    \n",
    "    # Calculate hexagon size\n",
    "    hex_size = np.sqrt(2 * target_area_km2 * 1e6 / (3 * np.sqrt(3)))  # Convert km² to m²\n",
    "    \n",
    "    # Calculate spacing\n",
    "    width = proj_bounds[2] - proj_bounds[0]\n",
    "    height = proj_bounds[3] - proj_bounds[1]\n",
    "    \n",
    "    # Horizontal spacing between hexagon centers\n",
    "    dx = hex_size * np.sqrt(3)\n",
    "    # Vertical spacing between hexagon centers\n",
    "    dy = hex_size * 1.5\n",
    "    \n",
    "    # Calculate number of columns and rows needed\n",
    "    cols = int(np.ceil(width / dx)) + 2  # Add buffer\n",
    "    rows = int(np.ceil(height / dy)) + 2  # Add buffer\n",
    "    \n",
    "    # Adjust bounds to ensure coverage\n",
    "    x_start = proj_bounds[0] - hex_size\n",
    "    y_start = proj_bounds[1] - hex_size\n",
    "    \n",
    "    hexagons = []\n",
    "    for row in range(rows):\n",
    "        # Offset every other row\n",
    "        if row % 2 == 0:\n",
    "            x_offset = 0\n",
    "        else:\n",
    "            x_offset = dx / 2\n",
    "            \n",
    "        for col in range(cols):\n",
    "            center_x = x_start + (col * dx) + x_offset\n",
    "            center_y = y_start + (row * dy)\n",
    "            \n",
    "            # Create hexagon in projected space\n",
    "            hex_poly = create_hexagon(center_x, center_y, hex_size)\n",
    "            # Transform back to geographic coordinates\n",
    "            geo_hex = transform(unproject, hex_poly)\n",
    "            hexagons.append(geo_hex)\n",
    "    \n",
    "    return gpd.GeoDataFrame({'geometry': hexagons}, crs='EPSG:4326')\n",
    "\n",
    "def print_region_stats(hex_grid):\n",
    "    \"\"\"Print detailed statistics for each region\"\"\"\n",
    "    print(\"\\nDetailed Region Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for region in sorted(hex_grid['region'].unique()):\n",
    "        region_data = hex_grid[hex_grid['region'] == region]\n",
    "        print(f\"\\nRegion: {region}\")\n",
    "        print(f\"Number of hexagons: {len(region_data)}\")\n",
    "        print(f\"Average area: {region_data['area_km2'].mean():.2f} km²\")\n",
    "        print(f\"Min area: {region_data['area_km2'].min():.2f} km²\")\n",
    "        print(f\"Max area: {region_data['area_km2'].max():.2f} km²\")\n",
    "        print(f\"Standard deviation: {region_data['area_km2'].std():.2f} km²\")\n",
    "\n",
    "def create_multi_region_hex_grid(regions, target_area_km2=100):\n",
    "    all_hexagons = []\n",
    "    grid_id = 1\n",
    "    \n",
    "    for region_name, coordinates in regions.items():\n",
    "        print(f\"Processing region: {region_name}\")\n",
    "        # Create region polygon\n",
    "        region_poly = Polygon(coordinates)\n",
    "        \n",
    "        # Generate hex grid for region bounds\n",
    "        hex_grid = generate_hex_grid(region_poly.bounds, target_area_km2)\n",
    "        \n",
    "        # Create region-specific equal area projection\n",
    "        center_lat = region_poly.centroid.y\n",
    "        center_lon = region_poly.centroid.x\n",
    "        proj_string = f\"+proj=aea +lat_1={center_lat-15} +lat_2={center_lat+15} +lat_0={center_lat} +lon_0={center_lon} +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n",
    "        \n",
    "        # Project region and hexagons for accurate intersection test and area calculation\n",
    "        project = pyproj.Transformer.from_crs('EPSG:4326', proj_string, always_xy=True).transform\n",
    "        proj_region = transform(project, region_poly)\n",
    "        \n",
    "        # Filter hexagons that intersect with region\n",
    "        region_hexagons = []\n",
    "        for hex_poly in hex_grid.geometry:\n",
    "            # Project hexagon to same CRS as region\n",
    "            proj_hex = transform(project, hex_poly)\n",
    "            \n",
    "            if proj_hex.intersects(proj_region):\n",
    "                centroid = hex_poly.centroid\n",
    "                # Calculate area in the projected CRS\n",
    "                area_km2 = proj_hex.area / 1e6  # Convert m² to km²\n",
    "                region_hexagons.append({\n",
    "                    'geometry': hex_poly,\n",
    "                    'grid_id': grid_id,\n",
    "                    'region': region_name,\n",
    "                    'centroid_lon': centroid.x,\n",
    "                    'centroid_lat': centroid.y,\n",
    "                    'area_km2': area_km2\n",
    "                })\n",
    "                grid_id += 1\n",
    "        \n",
    "        all_hexagons.extend(region_hexagons)\n",
    "        print(f\"Added {len(region_hexagons)} hexagons for {region_name}\")\n",
    "    \n",
    "    # Create GeoDataFrame with all hexagons\n",
    "    hex_grid = gpd.GeoDataFrame(all_hexagons, crs='EPSG:4326')\n",
    "    \n",
    "    return hex_grid\n",
    "\n",
    "# Define regions (your existing regions dictionary here)\n",
    "\n",
    "# Create grid for all regions\n",
    "hex_grid = create_multi_region_hex_grid(regions, target_area_km2=100)\n",
    "\n",
    "# Print detailed statistics\n",
    "print_region_stats(hex_grid)\n",
    "\n",
    "# Save separate files for each region\n",
    "for region_name in regions.keys():\n",
    "    region_grid = hex_grid[hex_grid['region'] == region_name]\n",
    "    region_grid.to_file(f\"{region_name}_hexgrid.shp\")\n",
    "    print(f\"Saved {region_name} with {len(region_grid)} hexagons\")\n",
    "\n",
    "print(f\"\\nOverall Results:\")\n",
    "print(f\"Average hexagon area: {hex_grid['area_km2'].mean():.2f} km²\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
